\chapter{Theory}

\section{Acoustic waves and waveguides}

In order to efficiently model the deformation and stresses in a solid material,
a linear elasticity model is often assumed.
For small deformations, solid materials obey Hooke's law which in it's full form
looks like
\[
	\sigma = C : \epsilon
\]
where $\sigma$ is the stress tensor, $C$ the elasticity tensor which is a
four-tensor that is a property of the material,
$\epsilon \coloneqq \nabla \vec u + (\nabla \vec u)^T$
is the strain tensor, and $:$ denotes double scalar product.
This equation is linear in $\vec u$, hence the name \emph{linear} elasticity.
Using this and newtons equations of motion, the equation governing the dynamics
is obtained:
\[
	\rho \ddot{\vec{u}} = \nabla \cdot \sigma + \vec F.
\]
where $\rho$ is the density, $\vec{u}$ is the displacement and $\vec F$ is the
externally applied force.
Assuming a time harmonic solution
$\vec u(\vec x, t) = \vec u(x) e^{i \omega t}$
with angular frequency $\omega$ this becomes
\[
	-\rho \omega^2 \vec{u} = \nabla \cdot \sigma + \vec F.
\]

To combine these into one equation that can be solved for $\vec{u}$ we first
rewrite them in index notation to make calculations clearer:
\begin{align}
	\epsilon_{ij} &= \frac12(\partial_i u_j + \partial_j u_i)\\
	\sigma_{ij} &= C_{ijkl} \epsilon_{kl}\\
				&= \frac12\left(C_{ijkl} \partial_k u_l + C_{ijkl} \partial_l
				u_k\right)\\
				&= C_{ijkl} \partial_k u_l\text{ because of the symmetry of $C$}
\end{align}
which gives
\begin{align}
	F_{i} &= -\rho \omega^2 u_{i} - \partial_j \sigma_{ij}\\
		   &= -\rho \omega^2 \delta_{ik} u_{k} -
		   \partial_j \left(C_{ijkl} \partial_l u_k\right)\\
		   &= -\left(\rho \omega^2 \delta_{ik} \placeholder + 
		   \partial_j \left(C_{ijkl} \partial_l \placeholder\right)\right) u_k
\end{align}
where the indices $i,j,k,l$ go over the spacial dimensions $x,y,z$.
All of the tensors in the equation above are really tensor fields, i.e.\ they are
functions of $\vec{x}$.
Defining the operator $\hat A_{ik}$ as
$-\left(\rho \omega^2 \delta_{ik} \placeholder + 
\partial_j \left(C_{ijkl} \partial_l \placeholder\right)\right)$
we can write
\begin{equation}
	\hat A_{ik} u_k = F_i
\end{equation}

\todowrt{
With no external forces we get traveling modes=eigenvalues.
Explain how and why periodicity means only certain modes can propagate.
Good resource in Chan's thesis, or maybe solid state physics book?
% Bloch state / Bloch's theorem
}

\tododec{At some point write about phonons? I haven't really had to care about
phonons so if I talk about it it's just for applications...}

\todowrt{Show our mode as example of this, and include band diagram}

\todowrt{Write about PML design and why we need it: Simulating infinite
	waveguides isn't possible because of the finite computing power. Also we
don't care about stuff far away, just that there are no reflections that can
interfere. Or maybe write about this in the method\ldots}

\section{Inverse Design}

Inverse design is a design paradigm where the design of a device is guided fully by
the desired characteristics.
These desired characteristics are quantified through what is called an objective
function%
\footnote{
	Also called \emph{figure of merit (FoM)} by some.%
}%
, which I will denote $\fobj$,
that should be maximized.
When coupled with \emph{adjoint simulation}, which is a clever way to compute
gradients, and gradient based optimization
algorithms, this is a very powerful methodology.

An overview of the design process is as follows:
\begin{enumerate}
	\item Initialize a random device design.
	\item\label{it:grad} Calculate the gradient of the design through the adjoint method.
	\item Update the device design using the gradient according to the optimization algorithm.
	\item If the device performance is good enough, terminate optimization, else
		return to step~\ref{it:grad}.
\end{enumerate}

\subsection{Adjoint Simulation}

Adjoint simulation is a way to compute the gradient of $\fobj$ with respect to
the design, which in our case means with respect to the material parameters.
I will in this section first give a general derivation, followed by the case of
inverse design in acoustics.

\subsubsection{General Derivation}\label{sec:general_derivation}

Let $\fobj$ be a function which depends on some (large) vector $v$.
The vector $v$ can be calculated by solving the linear equation
$A v = b$, where $b$ is a fixed vector and $A$ is a matrix that depends on a
vector of design parameters $p$.
The overall goal is to find the parameters $p$ that maximize the objective
function $\fobj$.
The goal of adjoint simulation is to find $\diff{\fobj}{p}$.
This can be expanded through the chain rule as
\[
	\diff{\fobj}{p} = \diff{\fobj}{v} \diff{v}{p}.
\]
To find the latter factor we do
\begin{align*}
	\diff{}{p} [Av = b] &\implies \diff{A}{p} v + A \diff{v}{p} = 0\\
						&\implies \diff{v}{p} = -A^{-1} \diff{A}{p} v
\end{align*}
which gives
\begin{align}
	\diff{\fobj}{p} &= - \diff{\fobj}{v} A^{-1} \diff{A}{p}
	v\label{eq:no_adj_sim}\\
	&= - \left(A^{-T} \diff{\fobj}{v}^T \right)^T \diff{A}{p} v
\end{align}
The first factor of this product is the solution to the \emph{adjoint problem}
\begin{equation}
	A^T \tilde v = \diff{\fobj}{v}^T,
\end{equation}
hence the name adjoint simulation.
As it turns out, $A$ is often symmetric which means that this is simply a normal
simulation but with $\difs{\fobj}{v}^T$ as the source.
Thus, to obtain the derivative we just need to run an additional
simulation with a different input.

Now you might be wondering: what have we gained by this?
Let $n$ be the dimension of $v$, $m$ the dimension of $p$ and $l$ the dimension
of $b$.
This means that $A$ is a matrix with dimension $l\times n$ and $\difs{A}{p}$ is
a three-tensor with dimension $m\times l\times n$.
Thus calculating $A^{-1} \difs{A}{p}$ directly involves solving $Ax = w$ for a
three-tensor, and calculating $A^{-1} \left(\difs{A}{p}\,v\right)$
involves solving for a matrix, both of which are orders of magnitude more
computationally expensive than solving for a vector.

\subsubsection{Specific derivation with acoustics}

Now we turn to the specific case of acoustic devices.
Here $A v = b$ is replaced by the dynamic equation \todowrt[noinline]{dynamic equation?
Better name} of linear elasticity:
\begin{equation}\label{eq:sim_eq}
	\hat A_{ik} u_k = F_i
\end{equation}
Instead of vectors, like we saw in \cref{sec:general_derivation}, these quantities are now functions%
\footnote{%
	Vector-valued funcitons, but that is not the important part here.
}
of $\vec x$.
Analogously to the vector of design parameters we now have a \emph{design field}
$p(\vec x)$.

\begin{tcolorbox}[title=On functionals and their derivatives, breakable,
	parbox=false]
	\tododec{
		Big fat box on functionals and their derivatives. I think this should be
		included somewhere, since very few of my peers know what a functional
		derivative is... Not really sure how though. I kinda like the thought of
		putting it in a box like this 
	}
Our $\fobj$ is no longer a function, but rather a \emph{functional}, and thus
we need to use the functional derivative instead of the ordinary derivative.
One can think of a functional as a function of a function,
i.e.\ something that maps an element of a function space to a scalar number.
There are also functionals which depend on both a function and a real number,
or on multiple functions.
Below I will give an overview of the notational conventions I use,
and then give the definition of the functional derivative as well as some useful
properties of it.

Let $\mathcal{Y}$ be a function space of functions $\R \to \R$.
A functional $F:\mathcal{Y} \to \R$ evaluated at the function
$f\in\mathcal{Y}$
is notated with the function in square brackets: $F[f]$.
If the functional additionally depends on a real number,
$G:\mathcal{Y} \times \R \to \R$,
that is put in round brackets: $G[f](x)$.
Note that in principle, $F$ is the functional while $F[f]$ is just a
number,
analogously to how $f$ is a function while $f(x)$ is a real number.

The functional derivative of $F$ with respect to it's function argument
is a functional $\mathcal{Y} \times \R \to \R$ denoted $\difs.f.{F[f]}{f}$.
In this expression, $f$ is technically a dummy function, writing
$\difs.f.{F[g]}{g}$ is exactly the same functional.
Almost always the functional derivative will be evaluated
at some function $h$.
In the stringent notation this is written
\begin{equation}
	\diff.f.{F[g]}{g} [h]
\end{equation}
and is simply an ordinary function $\R \to \R$.
However, I will use the notation $\difs.f.{F}{h}$ for this, to make the
equations clearer.\footnote{%
	This is consistent with how many other authors use the notation,
	though whether that is an \emph{intentional} notational simplification or
	not I will leave unsaid\ldots
}
\tododec{footnote should probably be deleted\ldots}
In addition, when evaluating the functional derivative at a point, the
argument can be put either after it, or in the denominator:
\begin{equation}
	\diff.f.{F[g]}{g} [h](x) = \diff.f.{F}{h}(x) = \diff.f.{F}{h(x)}.
\end{equation}

The functional derivative is defined by
\begin{equation}
	\int \diff.f.{F}{f} (x) \varphi(x) \dl x
	= \diff*{F[f + \varepsilon \varphi]}{\varepsilon}
\end{equation}
where $F$ is a functional of $f$ and $\varphi$ is an arbitrary test function.
\tododec{I find the definition somewhat difficult to comprehend and really
	didn't understand it until I sat down and did some examples for myself.
	Maybe talk about analogies to vector functions\ldots
}
I will use two properties of the functional derivative:
\begin{itemize}
	\item If $F$ is the functional $F[f](y) = f(y)$,
		then $\difs.f.{F(y)}{f}(x) = \delta(y-x)$.
	\item The chain rule: if $F$ is a functional with one function argument,
		$G$ is a functional with one function and one real argument,
		and $H$ is the functional defined as $H[f] = F[G[f](y)]$,
		then
		\begin{equation}
			\diff.f.{H}{f}(x)
			= \int \diff.f.{F}{G[f]}(y) \diff{G(y)}{f}(x) \dl{y}
		\end{equation}
\end{itemize}
\end{tcolorbox}

\todowrt{Look over what domains all of the integrals integrate over.}

For simplicity I will limit myself to the case where the objective function is
an overlap integral of the displacement field $u_k(\vec x)$ with some function
$\varphi_k^*(\vec x)$:
\begin{equation}
	\fobj[\vec u] = \int u_i(\vec x) \varphi_i^*(\vec x) \dl{\vec x}.
\end{equation}
Such an integral is a scalar product in the space where
$\vec u(\vec x)$ resides.
\tododec[noinline]{This integral would be over the unit cell, or over all of
space but with $\phi_i(\vec{x})$ being non-zero only inside the unit cell}

Analogously to the general derivation, we will use the chain rule to expand
$\difs.f.{\fobj}{p}(\vec x)$.
However, because $\vec u$ is in general complex, I will split it into its real and
imaginary components: $u_i = v_i + i w_i$.
\begin{equation}
	\label{eq:chain_rule}
	\diff.f.{f_\text{obj}}{p}(\bm x)
	=
	\int_\Omega \dl{\bm y} 
	\diff.f.{f_\text{obj}}{v_i}(\bm y)
	\diff.f.{v_i(\bm y)}{p}(\bm x)
	+
	\diff.f.{f_\text{obj}}{w_i}(\bm y)
	\diff.f.{w_i(\bm y)}{p}(\bm x)
\end{equation}
The first factor of each of the two terms is easy enough to calculate:
\begin{align}
	\diff.f.{f_\text{obj}}{v_i}(\bm y) &=
	\diff.f.*{
		\int_\Omega u_j(\bm x) \varphi_j^*(\bm x) \dl{\bm x}
	}{v_i(\bm y)}\\
	&= \int_\Omega
	\diff.f.*{
		u_j(\bm x) \varphi_j^*(\bm x)
	}{v_i(\bm y)} \dl{\bm x}\\
	&= \int_\Omega
	\delta(\bm x - \bm y) \delta_{ij} \varphi_j^*(\bm x)
	\dl{\bm x}\\
	&= \varphi_i^*(\bm y)
\end{align}
and
\begin{align}
	\diff.f.{f_\text{obj}}{w_i}(\bm y) &=
	\diff.f.*{
		\int_\Omega u_j(\bm x) \varphi_j^*(\bm x) \dl{\bm x}
	}{w_i(\bm y)}\\
	&= \int_\Omega
	\diff.f.*{
		u_j(\bm x) \varphi_j^*(\bm x)
	}{w_i(\bm y)} \dl{\bm x}\\
	&= \int_\Omega
	i \delta(\bm x - \bm y) \delta_{ij} \varphi_j^*(\bm x)
	\dl{\bm x}\\
	&= i \varphi_i^*(\bm y)
\end{align}
which gives us
\begin{align}
	\diff.f.{f_\text{obj}}{p}(\bm x)
	&=
	\int_\Omega \dl{\bm y}
	\varphi_i^*(\bm{y})
	\diff.f.{v_i(\bm y)}{p}(\bm x)
	+
	i \varphi_i^*(\bm{y})
	\diff.f.{w_i(\bm y)}{p}(\bm x)\\
	&=
	\int_\Omega \dl{\bm y}
	\varphi_i^*(\bm{y})
	\Re\left(\diff.f.{u_i(\bm y)}{p}(\bm x)\right)
	+
	i \varphi_i^*(\bm{y})
	\Im\left(\diff.f.{u_i(\bm y)}{p}(\bm x)\right)\\
	&=
	\int_\Omega \dl{\bm y}
	\varphi_i^*(\bm{y})
	\diff.f.{u_i(\bm y)}{p}(\bm x)
	\label{eq:dfdp_as_dudp_integral}
\end{align}
\tododec{I am mixing putting the argument in the denominator / after the
functional derivative. Should I be consistent? I like after more, but it is a
bit weird when putting the operant after the derivative}
To find $\difs.f.{u_i(\bm y)}{p}(\bm x)$ we apply $\difs.f.{}{p(\bm x)}$ to
equation~\eqref{eq:sim_eq}, which gives us
\begin{equation}\label{eq:dAdpu_Adudp}
	0 =
	\diff.f.{\hat A_{ik}}{p}(\vec x) u_k(\bm y)
	+
	\hat A_{ik} \diff.f.{u_k(\bm y)}{p(\bm x)}
\end{equation}

The point of inverse design is that we now want to find an ajoint field
$\tilde u_i(\bm y)$
such that the integral in equation~\eqref{eq:dfdp_as_dudp_integral} is
\begin{equation}\label{eq:phi_dudp_integral}
	\int_\Omega \dl{\bm y}\,
	\varphi_i^*(\bm y)
	\diff.f.{u_i(\bm y)}{p}(\bm x)
	=
	\int_\Omega \dl{\bm y}\,
	\tilde u_i(\bm y)
	\hat A_{ik}
	\diff.f.{u_k(\bm y)}{p}(\bm x)
\end{equation}
which by equation~\cref{eq:dAdpu_Adudp} is equal to
\begin{equation}
	\int_\Omega \dl{\bm y}\,
	\tilde u_i(\bm y)
	\diff.f.{\hat A_{ik}}{p}(\vec x)
	u_k(\bm y).
\end{equation}
\todowrt{use the form of $\hat A$ to obtain an explicit formula for this}
The way to find this $\tilde u_i$ is through an adjoint simulation.

\begin{tcolorbox}
	When discretizing space \cref{eq:phi_dudp_integral} becomes
	\begin{equation}
		\varphi_{a}^* \eta_{ab} = \tilde u_{c} A_{ca} \eta_{ab}
	\end{equation}
	Meaning that to find $\tilde u$ we solve
	\begin{equation}
		\varphi_a^* = \tilde u_c A_{ca}
	\end{equation}
	or, taking the hermitian adjoint of both sides
	\begin{equation}
		A_{ac}^\dagger \tilde u_c^* = \varphi_a
	\end{equation}
	which in normal matrix notation is
	\begin{equation}
		A^\dagger \tilde u^* = \varphi
	\end{equation}
\end{tcolorbox}

\todoblk{The infamous integral. Still am not entirely sure about it, but I
haven't thought more about it since I abandoned that.}

\subsection{Optimization Algorithms}

In the last section I painstakingly derived how one can obtain the gradient,
and in this section I will attempt to justify that by describing how one can use
the gradient.
I will begin by describing the advantages of gradient based optimization
algorithms over those that don't use the gradient.
Following that I describe the algorithm that I used, as well as some of it's
predecessors.

An optimization algorithm is an algorithm for finding the optimum of a function.
The function is often called the \emph{objective function} or the \emph{cost
function}.
A very naive optimization method would be to simply try some number of inputs
and then choose the one with the highest function value.
This would require a large number of points before a good value is found,
which means that it would take a long time.
An improvement to this method is to use the information gained from the points
already tried to decide which points to try next.
If some point has a bad value, then try somewhere else; if some point has a good
value, try another close by.
Examples of algorithms that do this are bayesian optimization, particle swarm
optimization, \todowrt[noinline]{more examples}
However, if the input to your function is very high-dimensional,
choosing a random direction step in for your next try is very unlikely
to yield a much better value.
For such functions, it is essential that you know in which direction the
function increases the fastest, so that you can choose your next point in that
direction. That is why we want to use gradient based optimization algorithms;
they enable us to quickly find the right direction to go in for best
improvement.

\tododec{%
	Cool thing to maybe put in. If you have a linear function
	$f(x) = v\cdot x + k$, what is the probability that you will significantly
	(say more than 10\% of optimal step) increase $f$ with a random unit
	length step?
	Optimal: step in $\hat v$, $\Delta f = v\cdot \hat v = \norm{v}$.
	Probability of 10\% of this: $\int_{0.1}^1 (1-x^2)^{n/2} \dl x / \int_{-1}^1
	\cdots$
	use $(1-x^2)^n \approx \exp(-n x^2)$.
}

\subsubsection{Gradient Descent}

The simplest gradient based optimization algorithm is called \emph{gradient
descent}.
Like all of the algorithms I will describe it is an iterative algorithm,
meaning that it generates a sequence of points that converges to an optimum,
and the next point in the sequence is derived from the previous ones.
In the case of gradient descent, the next point is gotten by
\begin{equation}
	p_n = p_{n-1} + \eta g_{n-1}
\end{equation}
where $\eta$ is the so called \emph{learning rate} and $g_{n-1}$ is the gradient
of the objective function at $p_{n-1}$.
For ordinary gradient descent the learning rate would be fixed,
and choosing an appropriate value for this parameter is one of the problems of
this method.
If a too high value is chosen, then the steps taken will be too large and the
optimium might be missed entirely.
A too low value results in too small steps which will yield a slow convergence.

\subsubsection{Adaptive Moment Estimation (ADAM)}

\todowrt{Paragraph describing the \gls{adam} algorithm}

\todowrt{\gls{adam} has one learning rate for each dimension}
\todowrt{\gls{adam} one does not need to manually set the learning rate}
\tododec{Somewhere I should write about epsilon, and alpha and how I set them,
but that probably needs to come later, in the method}
